---
---

@inproceedings{mansourian2023multi,
  title={Multi-task Learning for Joint Re-identification, Team Affiliation, and Role Classification for Sports Visual Tracking},
  author={Mansourian, Amir M and Somers, Vladimir and De Vleeschouwer, Christophe and Kasaei, Shohreh},
  booktitle={Proceedings of the 6th International Workshop on Multimedia Content Analysis in Sports},
  pages={103--112},
  year={2023}
}

@article{cioppa2023soccernet,
  title={SoccerNet 2023 Challenges Results},
  author={Cioppa, Anthony and Giancola, Silvio and Somers, Vladimir and Magera, Floriane and Zhou, Xin and Mkhallati, Hassan and Deliege, Adrien and Held, Jan and Hinojosa, Carlos and Mansourian, Amir M and others},
  journal={arXiv preprint arXiv:2309.06006},
  year={2023}
}

@article{mansourian2023aicsd,
  title={AICSD: Adaptive Inter-Class Similarity Distillation for Semantic Segmentation},
  author={Mansourian, Amir M and Ahmadi, Rozhan and Kasaei, Shohreh},
  journal={arXiv preprint arXiv:2308.04243},
  year={2023}
}

@article {
author = {Mansourian, Amir and Karimi, Nader and Kasaei, Shohreh},
title = {An Efficient Knowledge Distillation Architecture for Real-time Semantic Segmentation},
journal = {AUT Journal of Modeling and Simulation},
volume = {55},
number = {1},
pages = {6-6},
year  = {2023},
publisher = {Amirkabir University of Technology},
issn = {2588-2953}, 
eissn = {2588-2961}, 
doi = {10.22060/miscj.2023.21949.5307},
abstract = {In recent years, Convolutional Neural Networks (CNNs) have made significant strides in the field of segmentation, particularly in semantic segmentation where both accuracy and efficiency are crucial. However, despite their high accuracy, these deep networks are not practical for real-time use due to their low inference speed. This issue has prompted researchers to explore various techniques to improve the efficiency of CNNs. One such technique is knowledge distillation, which involves transferring knowledge from a larger, cumbersome (teacher) model to a smaller, more compact (student) model. This paper proposes a simple yet efficient approach to address the issue of low inference speed in CNNs using knowledge distillation. The proposed method involves distilling knowledge from the feature maps of the teacher model to guide the learning of the student model. The approach uses a straightforward technique known as pixel-wise distillation to transfer the feature maps of the last convolution layer of the teacher model to the student model. Additionally, a pair-wise distillation technique is used to transfer pair-wise similarities of the intermediate layers. To validate the effectiveness of the proposed method, extensive experiments were conducted on the PascalVoc 2012 dataset using a state-of-the art DeepLabV3+ segmentation network with different backbone architectures. The results showed that the proposed method achieved a balanced mean Intersection over Union (mIoU) and training time.},
keywords = {Convolutional Neural Networks,Semantic Segmentation,Knowledge Distillation},	
url = {https://miscj.aut.ac.ir/article_5292.html},
eprint = {}
}
